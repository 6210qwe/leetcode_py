# -*- coding: utf-8 -*-
"""
@Time   : 2026/2/2
@Author : zhang
@Desc   : ä»…descriptionä¸ºç©ºæ—¶è¡¥å…¨ï¼ˆåŒæ—¶è¡¥æè¿°+æ ‡ç­¾ï¼‰ï¼Œæœ¬åœ°æ•°æ®å†™ä¿æŠ¤
"""
import sys
import re
import json
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
from lxml import etree

# é¡¹ç›®æ ¹è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


# ä»£ç†é…ç½®
def get_proxy():
    tunnel = "g184.kdltps.com:15818"
    username = "t13632437348639"
    password = "10cc7lx7"
    return {
        "http": f"http://{username}:{password}@{tunnel}",
        "https": f"http://{username}:{password}@{tunnel}",
    }


# æå–é¢˜ç›®æè¿°
def extract_description_from_html(html_content):
    try:
        parser = etree.HTMLParser()
        tree = etree.fromstring(html_content.encode("utf-8"), parser)
        nodes = tree.xpath("//meta[@name='description']/@content")
        if not nodes:
            return ""
        desc = (nodes[0] or "").strip()
        return re.sub(r"\s+", " ", desc).strip() or ""
    except Exception as e:
        print(f"æå–æè¿°å¤±è´¥: {e}")
        return ""


# æå–æ ‡ç­¾
def extract_topic_tags_from_html(html_content):
    try:
        pattern = r'"topicTags"\s*:\s*(\[[\s\S]*?\])'
        m = re.search(pattern, html_content, re.DOTALL)
        if m:
            arr = json.loads(m.group(1))
            if isinstance(arr, list):
                return [
                    {
                        "name": item.get("name"),
                        "slug": item.get("slug", "").strip(),
                        "translatedName": item.get("translatedName"),
                    }
                    for item in arr
                    if isinstance(item, dict) and item.get("slug", "").strip()
                ]
    except Exception as e:
        print(f"æå–æ ‡ç­¾å¤±è´¥: {e}")
    return []


# æ„å»ºtopicså­—æ®µ
def build_topics_from_topic_tags(topic_tags):
    topics = []
    for t in topic_tags:
        name = (t.get("translatedName") or t.get("name") or t.get("slug") or "").strip()
        if name and name not in topics:
            topics.append(name)
    return topics


# GraphQLå…œåº•è·å–æ ‡ç­¾
def fetch_topic_tags_from_graphql(session, proxies, slug):
    query = """
    query questionData($titleSlug: String!) {
      question(titleSlug: $titleSlug) {
        topicTags { name slug translatedName }
      }
    }
    """
    try:
        resp = session.post(
            "https://leetcode.cn/graphql/",
            json={"query": query, "variables": {"titleSlug": slug}},
            proxies=proxies,
            timeout=20,
            headers={"Content-Type": "application/json"},
        )
        resp.raise_for_status()
        tags = resp.json().get("data", {}).get("question", {}).get("topicTags", [])
        return [
            {
                "name": t.get("name"),
                "slug": t.get("slug", "").strip(),
                "translatedName": t.get("translatedName"),
            }
            for t in tags
            if isinstance(t, dict) and t.get("slug", "").strip()
        ]
    except Exception as e:
        print(f"GraphQLè·å–æ ‡ç­¾å¤±è´¥ {slug}: {e}")
        return []


# æŠ“å–å•é¢˜ï¼šåŒæ—¶æ‹¿æè¿°+æ ‡ç­¾
def fetch_question_detail(session, proxies, slug):
    url = f"https://leetcode.cn/problems/{slug}/"
    for attempt in range(3):
        try:
            r = session.get(url, proxies=proxies, timeout=15)
            r.raise_for_status()
            desc = extract_description_from_html(r.text)
            tags = extract_topic_tags_from_html(r.text)
            # æ ‡ç­¾ä¸ºç©ºæ—¶èµ°GraphQLå…œåº•
            tags = tags if tags else fetch_topic_tags_from_graphql(session, proxies, slug)
            return desc, tags
        except Exception as e:
            print(f"æŠ“å–è¯¦æƒ…å¤±è´¥ {slug} ç¬¬{attempt+1}æ¬¡: {e}")
            time.sleep(0.6 * (attempt + 1))
    return "", []


class LeetCodeDataUpdater:
    def __init__(self):
        self.use_proxy = True
        self.max_workers = 10
        self.output_file = project_root / "leetcode.json"
        self.proxies = get_proxy() if self.use_proxy else None

        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36",
            "Accept-Language": "zh-CN,zh;q=0.9",
        })

        # å†™ä¿æŠ¤å­—æ®µï¼šæœ¬åœ°æœ‰å€¼å°±æ°¸ä¸è¦†ç›–
        self.protected_fields = ["description", "topicTags", "topics", "used"]

    # åŠ è½½æœ¬åœ°æ•°æ®å¹¶å»é‡
    def load_and_clean_local_data(self):
        if not self.output_file.exists():
            print("æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–ç©ºæ•°æ®")
            return {"stat_status_pairs": []}, {}

        try:
            with open(self.output_file, "r", encoding="utf-8") as f:
                raw_data = json.load(f)
        except Exception as e:
            print(f"è¯»å–æœ¬åœ°æ–‡ä»¶å¤±è´¥ï¼š{e}")
            return {"stat_status_pairs": []}, {}

        if not isinstance(raw_data, dict) or "stat_status_pairs" not in raw_data:
            print("æœ¬åœ°æ•°æ®ç»“æ„ä¸åˆæ³•ï¼Œåˆå§‹åŒ–ç©ºæ•°æ®")
            return {"stat_status_pairs": []}, {}

        clean_pairs = []
        local_qid_map = {}
        seen_qids = set()

        for item in raw_data["stat_status_pairs"]:
            qid = item.get("stat", {}).get("question_id")
            if qid is None or not isinstance(qid, int):
                continue

            if qid not in seen_qids:
                seen_qids.add(qid)
                clean_pairs.append(item)
                # ç¼“å­˜æœ¬åœ°ä¿æŠ¤å­—æ®µ
                local_qid_map[qid] = {
                    k: item.get(k, "" if k != "topicTags" else [])
                    for k in self.protected_fields
                }

        print(f"æœ¬åœ°æ•°æ®å»é‡åï¼š{len(clean_pairs)} æ¡ï¼Œå”¯ä¸€ qidï¼š{len(local_qid_map)}")
        return {"stat_status_pairs": clean_pairs}, local_qid_map

    # è·å–æ¥å£æœ€æ–°æ•°æ®
    def fetch_latest_api_data(self):
        try:
            resp = self.session.get(
                "https://leetcode.cn/api/problems/all/",
                proxies=self.proxies,
                timeout=30
            )
            resp.raise_for_status()
            data = resp.json()
            if not isinstance(data, dict) or "stat_status_pairs" not in data:
                raise ValueError("æ¥å£è¿”å›ç¼ºå°‘ stat_status_pairs")
            print(f"æ¥å£æ•°æ®ï¼š{len(data['stat_status_pairs'])} æ¡")
            return data
        except Exception as e:
            print(f"è·å–æ¥å£æ•°æ®å¤±è´¥ï¼š{e}")
            raise

    # å®‰å…¨åˆå¹¶ï¼šæœ¬åœ°ä¿æŠ¤å­—æ®µç»ä¸è¦†ç›–
    def merge_data_with_protection(self, local_data, local_qid_map, api_data):
        api_qid_map = {}
        for item in api_data["stat_status_pairs"]:
            qid = item.get("stat", {}).get("question_id")
            if qid is not None and isinstance(qid, int) and qid not in api_qid_map:
                api_qid_map[qid] = item

        merged_pairs = []

        # æ¥å£å­˜åœ¨çš„é¢˜ç›®ï¼šåŸºç¡€æ•°æ®ç”¨æ¥å£ï¼Œä¿æŠ¤å­—æ®µç”¨æœ¬åœ°
        for qid, api_item in api_qid_map.items():
            merged = api_item.copy()
            if qid in local_qid_map:
                merged.update(local_qid_map[qid])
            else:
                # æ–°é¢˜åˆå§‹åŒ–
                merged["description"] = ""
                merged["topicTags"] = []
                merged["topics"] = []
                merged["used"] = 0
            merged_pairs.append(merged)

        # æœ¬åœ°ç‹¬æœ‰é¢˜ç›®å®Œæ•´ä¿ç•™
        local_only = set(local_qid_map.keys()) - set(api_qid_map.keys())
        for qid in local_only:
            for item in local_data["stat_status_pairs"]:
                if item.get("stat", {}).get("question_id") == qid:
                    merged_pairs.append(item)
                    break

        merged_data = api_data.copy()
        merged_data["stat_status_pairs"] = merged_pairs
        print(f"åˆå¹¶å®Œæˆï¼šæ€»è®¡ {len(merged_pairs)} æ¡")
        return merged_data

    # ä¿å­˜æ•°æ®
    def save_data(self, data, tip=""):
        try:
            self.output_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.output_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            if tip:
                print(f"âœ… {tip}")
        except Exception as e:
            print(f"ä¿å­˜å¤±è´¥ï¼š{e}")
            raise

    # æ ¸å¿ƒé€»è¾‘ï¼šä»…descriptionä¸ºç©ºæ—¶ï¼Œè¡¥å…¨æè¿°+æ ‡ç­¾
    def enrich_detail_when_desc_empty(self, data):
        pairs = data["stat_status_pairs"]
        jobs = []

        # ç­›é€‰è§„åˆ™ï¼šä»…descriptionä¸ºç©ºæ—¶æ‰è¡¥å…¨
        for idx, item in enumerate(pairs):
            if not isinstance(item, dict):
                continue
            stat = item.get("stat", {})
            qid = stat.get("question_id")
            slug = stat.get("question__title_slug")

            if qid is None or not slug or not isinstance(slug, str):
                continue
            slug = slug.strip()

            # å”¯ä¸€åˆ¤æ–­æ¡ä»¶ï¼šdescriptionä¸ºç©º
            if not item.get("description"):
                jobs.append((idx, qid, slug))
                print(f"é¢˜ç›® {qid} ({slug})ï¼šdescriptionä¸ºç©ºï¼Œéœ€è¡¥å…¨æè¿°+æ ‡ç­¾")

        print(f"\néœ€è¦è¡¥å…¨çš„é¢˜ç›®æ€»æ•°ï¼š{len(jobs)}")
        if not jobs:
            return data

        ok = 0
        fail = 0

        # å¤šçº¿ç¨‹è¡¥å…¨ï¼ˆåŒæ—¶è¡¥æè¿°+æ ‡ç­¾ï¼‰
        with ThreadPoolExecutor(max_workers=self.max_workers) as ex:
            future_map = {
                ex.submit(fetch_question_detail, self.session, self.proxies, slug): (idx, qid, slug)
                for idx, qid, slug in jobs
            }

            for i, fut in enumerate(as_completed(future_map), 1):
                idx, qid, slug = future_map[fut]
                desc, tags = fut.result()
                topics = build_topics_from_topic_tags(tags)

                # ä»…å½“descriptionä¸ºç©ºæ—¶æ›´æ–°ï¼ˆåŒé‡ä¿æŠ¤ï¼‰
                if not pairs[idx].get("description"):
                    pairs[idx]["description"] = desc
                    pairs[idx]["topicTags"] = tags
                    pairs[idx]["topics"] = topics

                # ç»Ÿè®¡ç»“æœ
                if desc or tags:
                    ok += 1
                    print(f"[{i}/{len(jobs)}] âœ… è¡¥å…¨æˆåŠŸ {qid} ({slug})ï¼šæè¿°={len(desc)}å­—ç¬¦ï¼Œæ ‡ç­¾={len(tags)}ä¸ª")
                else:
                    fail += 1
                    print(f"[{i}/{len(jobs)}] âŒ è¡¥å…¨å¤±è´¥ {qid} ({slug})ï¼šæ— æœ‰æ•ˆæ•°æ®")

                # æ¯50é¢˜åˆ†æ‰¹ä¿å­˜
                if i % 50 == 0:
                    self.save_data(data, tip=f"åˆ†æ‰¹ä¿å­˜ï¼šå·²å¤„ç† {i}/{len(jobs)} é¢˜")
                    print(f"è¿›åº¦ï¼šæˆåŠŸ {ok} | å¤±è´¥ {fail}")

        # æœ€ç»ˆä¿å­˜
        self.save_data(data, tip=f"è¡¥å…¨å®Œæˆï¼šæˆåŠŸ {ok} é¢˜ï¼Œå¤±è´¥ {fail} é¢˜")
        return data

    # ä¸»æµç¨‹
    def run(self):
        try:
            print("===== åŠ è½½æœ¬åœ°æ•°æ® =====")
            local_data, local_qid_map = self.load_and_clean_local_data()
            print("\n===== è·å–æ¥å£æ•°æ® =====")
            api_data = self.fetch_latest_api_data()
            print("\n===== å®‰å…¨åˆå¹¶ï¼ˆå†™ä¿æŠ¤ï¼‰ =====")
            merged_data = self.merge_data_with_protection(local_data, local_qid_map, api_data)
            self.save_data(merged_data, tip="åˆå¹¶å®Œæˆï¼Œæœ¬åœ°è¯¦æƒ…å·²ä¿æŠ¤")
            print("\n===== è¡¥å…¨descriptionä¸ºç©ºçš„é¢˜ç›®ï¼ˆåŒæ—¶è¡¥æè¿°+æ ‡ç­¾ï¼‰ =====")
            final_data = self.enrich_detail_when_desc_empty(merged_data)
            print("\nğŸ‰ å…¨éƒ¨å®Œæˆï¼è§„åˆ™ï¼šä»…descriptionä¸ºç©ºæ—¶è¡¥å…¨ï¼ˆæè¿°+æ ‡ç­¾ï¼‰ï¼Œæœ¬åœ°æ•°æ®100%ä¿ç•™")
        except Exception as e:
            print(f"\nâŒ æ‰§è¡Œå¤±è´¥ï¼š{e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)


if __name__ == "__main__":
    updater = LeetCodeDataUpdater()
    updater.run()